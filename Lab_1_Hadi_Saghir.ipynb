{"cells":[{"cell_type":"markdown","metadata":{"id":"nAUA6cvb_CNY"},"source":["# 5TF078 Deep Learning Course\n","## Excercise 1 Convolutional Neural Networks on Fashion-MNIST\n","Created by Tomas Nordström, Umeå University\n","\n","Revisions:\n","* 2022-03-23 First revision based on earlier excercises /ToNo\n","* 2022-03-30 Adjusted the Keras Tuner search to reduce compute times. /Tomas\n","* 2022-04-05 Added padding='same' as an argument to Conv2D. /Tomas\n","* 2022-05-09 Swaped optimizer to use 'adam' as default and 'sgd' as extra test. /Tomas\n","* 2022-05-09 Fixed so that we can use the latest version (1.1.2) of keras tuner. /Tomas\n","* 2022-11-07 Added hints to use incremental model definitions when we want to have varying number of layers with Keras tuner. /Tomas"]},{"cell_type":"markdown","metadata":{"id":"IAWshxTD0Sd8"},"source":["## **Hadi Saghir**"]},{"cell_type":"markdown","metadata":{"id":"r-tp9iDDCpsz"},"source":["# First we initilize our Python environment"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3240,"status":"ok","timestamp":1668606797772,"user":{"displayName":"Hadi ammar mohammad Saghir","userId":"05748174023284280158"},"user_tz":-60},"id":"F1pcr2Em_Lzo","outputId":"bad5fc41-bb4b-4775-8ec7-2cebdab9b806"},"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow version: 2.9.2\n","Keras version: 2.9.0\n"]}],"source":["# Import needed libraries\n","import tensorflow as tf\n","print('TensorFlow version:', tf.__version__)\n","\n","# from tensorflow import keras\n","# from tensorflow.keras import layers\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten, Conv2D\n","from tensorflow.keras.utils  import to_categorical\n","\n","print('Keras version:',tf.keras.__version__)\n","\n","# Helper libraries\n","import numpy as np\n","import sklearn\n","from   sklearn.model_selection import train_test_split\n","\n","# Matlab plotting\n","import matplotlib\n","import matplotlib.pyplot as plt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":460,"status":"ok","timestamp":1668606798215,"user":{"displayName":"Hadi ammar mohammad Saghir","userId":"05748174023284280158"},"user_tz":-60},"id":"Uf0ZWo9JyEhS","outputId":"cfb4d73a-57f5-46a4-ad8d-cb4882942096"},"outputs":[{"output_type":"stream","name":"stdout","text":["GPU 0: Tesla T4 (UUID: GPU-1f41b3c8-35d4-5d44-77bf-a942d8a57de8)\n"]}],"source":["# Test for GPU and determine what GPU we have\n","import sys\n","if not tf.config.list_physical_devices('GPU'):\n","     print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n","     IN_COLAB = 'google.colab' in sys.modules\n","     if IN_COLAB:\n","         print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n","else:\n","     !nvidia-smi -L"]},{"cell_type":"markdown","metadata":{"id":"AhWTXQfXBiN6"},"source":["# Set up the needed data sets"]},{"cell_type":"markdown","metadata":{"id":"kmtboE3kolMi"},"source":["## Get hold of a data-set\n","In this exercise we will use Fashion MNIST dataset, which an alternative to MNIST (it is a little harder, but the image size is the same). This is available directly as a [Keras dataset](https://keras.io/datasets/). This is a dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. \n","\n","Note that we split our data into **three** data sets: training, validation, testing; each with its own purpose."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2526,"status":"ok","timestamp":1668606801846,"user":{"displayName":"Hadi ammar mohammad Saghir","userId":"05748174023284280158"},"user_tz":-60},"id":"0ViGcOSI-_t6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fff3127c-df7b-4791-ca0e-02e17f2e894f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","29515/29515 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","26421880/26421880 [==============================] - 2s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","5148/5148 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","4422102/4422102 [==============================] - 0s 0us/step\n"]}],"source":["# Get Fashion-MNIST training and test data from Keras database (https://keras.io/datasets/)\n","(train_images0, train_labels0), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n","\n","# Define labels\n","class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n","               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n","\n","# Split the training set into a training and a validation set (20% is validation)\n","train_images, val_images, train_labels, val_labels = train_test_split(train_images0, train_labels0, test_size=0.20)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1668606801847,"user":{"displayName":"Hadi ammar mohammad Saghir","userId":"05748174023284280158"},"user_tz":-60},"id":"GNZOs2YwraHQ","outputId":"d05e1a75-b743-4626-a42e-950285c0649d"},"outputs":[{"output_type":"stream","name":"stdout","text":["No training images: 48000  with image size: 28 x 28\n","No test images: 10000\n","No val images: 12000\n","Training labels: [0 1 2 3 4 5 6 7 8 9] ; That is, 10 classes.\n"]}],"source":["# Print som basic information of data set sizes and data sizes\n","train_no,x,y = train_images.shape\n","print('No training images:',train_no, ' with image size:',x,'x',y)\n","label_no = len(train_labels)\n","if (label_no != train_no) : \n","  print('# labels do not match # training images')\n","\n","test_no,x,y = test_images.shape\n","label_no = len(test_labels)\n","print('No test images:',test_no)\n","if (label_no != test_no) : \n","  print('# labels do not match # test images')\n","\n","val_no,x,y = val_images.shape\n","label_no = len(val_labels)\n","print('No val images:',val_no)\n","if (label_no != val_no) : \n","  print('# labels do not match # val images')\n","\n","classes = np.unique(train_labels)\n","num_classes = len(classes)\n","print('Training labels:', np.unique(train_labels), \"; That is,\", num_classes,\"classes.\" )\n"]},{"cell_type":"markdown","metadata":{"id":"Cg4WF2gMzfpm"},"source":["Note that the training labels are an integer between 0 and 9, which is not very good as outputs (or inputs) for DL models. A better approach would be to use a one-hot encoding. We can convert our label vectors to one-hot encoded matrices by using [to_categorical](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical) `tf.keras.utils.to_categorical(train_labels)`.\n","\n","But we can achieve the same thing by using [SparseCategoricalCrossentropy](https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class) as a loss function instead of the [CategoricalCrossentropy](https://keras.io/api/losses/probabilistic_losses/#categoricalcrossentropy-class). Also note that in both cases we need our model to have as many output nodes as we have classes! "]},{"cell_type":"markdown","metadata":{"id":"gYVB7wI8sFgh"},"source":["## Adjust the data to be better work as ML input\n","\n","Many models working with images are assuming the data to be represented as a 4-D tensor with the shape BHWC [batch_size, height, width, channels] (some ML frameworks prefer to use BCHW instead, so be careful when starting to work on new datasets or ML-frameworks).\n","\n","We also want to normalize data to be \"small\" and \"close\" to zero, e.g. 0 to 1 or –1 to 1. In this example we normalize to values between –0.5 and 0.5."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XL5_y51yqOkR"},"outputs":[],"source":["# Add an \"empty\" color dimension for our data sets\n","train_images = np.expand_dims(train_images, -1)\n","val_images = np.expand_dims(val_images, -1)\n","test_images = np.expand_dims(test_images, -1)\n","\n","# Normalize the images.\n","train_images = (train_images / 255) - 0.5\n","test_images = (test_images / 255) - 0.5\n","val_images = (val_images / 255) - 0.5"]},{"cell_type":"markdown","metadata":{"id":"P9F4zkQQDnAz"},"source":["## Explore the data\n","It is always advised to take a look at the data, to see if we need to massage it further."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":549},"executionInfo":{"elapsed":516,"status":"ok","timestamp":1668300848073,"user":{"displayName":"Hadi ammar mohammad Saghir","userId":"05748174023284280158"},"user_tz":-60},"id":"dMYGkeRxJ__U","outputId":"738abf52-f82c-444b-823b-241529e160f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Label: Sneaker\n"]},{"data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Label: Coat\n"]},{"data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# As these are images (28x28) it can be interesting to plot some as images\n","image_index = [42, 789] # \"Random\" images to print\n","\n","for index in image_index:\n","  print( 'Label:', class_names[train_labels[index]])\n","  plt.figure()\n","  plt.imshow(np.squeeze(train_images[index], axis=-1))\n","  plt.gray()\n","  plt.grid(False)\n","  plt.show(block=False)"]},{"cell_type":"markdown","metadata":{"id":"H7CFf7AO4ysm"},"source":["# Define a convolutional network model"]},{"cell_type":"markdown","metadata":{"id":"948BTPYO5BI_"},"source":["## Define the model using Keras\n","\n","Note that this is a *very* small modell just to have a strating point. A good modell is expected to have 5-50 times as many parameters!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":234,"status":"ok","timestamp":1668343401272,"user":{"displayName":"Hadi ammar mohammad Saghir","userId":"05748174023284280158"},"user_tz":-60},"id":"NHl97r_t4_2t","outputId":"f3236cb7-9611-40ad-ae3b-c4ec7e17d796"},"outputs":[{"name":"stdout","output_type":"stream","text":["Input shape (28, 28, 1)\n","Model: \"sequential_13\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_65 (Conv2D)          (None, 28, 28, 16)        160       \n","                                                                 \n"," flatten_15 (Flatten)        (None, 12544)             0         \n","                                                                 \n"," dense_36 (Dense)            (None, 16)                200720    \n","                                                                 \n"," dense_37 (Dense)            (None, 10)                170       \n","                                                                 \n","=================================================================\n","Total params: 201,050\n","Trainable params: 201,050\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# We need to give the input shape (i.e. our image shape) to our model\n","input_shape = test_images[0].shape\n","print(\"Input shape\", input_shape)\n","\n","# The Keras model will be the simplest Keras model for NN networks. \n","# It is a single stack of layers connected sequentially.\n","model = Sequential([\n","\n","# Add a convolution layer\n","Conv2D(filters=16, kernel_size=(3, 3), padding='same', activation='relu', input_shape=input_shape),\n","\n","# Flatten the input. This prepares the vector for fully connected layers.\n","Flatten(),\n","\n","# Add a hidden Dense layer\n","Dense(units=16, activation='relu'),\n","\n","# Add a an output layer. The output space is the number of classes\n","#    Softmax makes the output as probablity vector of the different classes\n","Dense(units=num_classes, activation='softmax')\n","\n","])\n","\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"7opTLcKc73By"},"source":["# Set up the model and optimizer for training\n","To set up the optimization of this model we need to compile it, and set what [optimizer](https://keras.io/api/optimizers/), what [loss](https://keras.io/api/losses/), and what [metrics](https://keras.io/api/metrics/) to use. Where, metric is similar to a loss but not used during training but can be used to measure training progress.\n","\n","The model (its parameters) is also initialized to some random values during this phase."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"luIM0Q8I9I9_"},"outputs":[],"source":["# Compile the model, as a preparation for training\n","model.compile(\n","  optimizer='adam',\n","  loss='categorical_crossentropy',\n","  metrics=['categorical_accuracy']\n",")"]},{"cell_type":"markdown","metadata":{"id":"q56fJLY89amp"},"source":["# Run the training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":582},"executionInfo":{"elapsed":120377,"status":"error","timestamp":1668607273135,"user":{"displayName":"Hadi ammar mohammad Saghir","userId":"05748174023284280158"},"user_tz":-60},"id":"fnJObQvJ-Gv7","outputId":"0966d73b-8a29-45c4-eb2f-503ab77e0c3d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","1500/1500 [==============================] - 28s 13ms/step - loss: 0.9306 - categorical_accuracy: 0.6536 - val_loss: 0.4733 - val_categorical_accuracy: 0.8249\n","Epoch 2/15\n","1500/1500 [==============================] - 18s 12ms/step - loss: 0.6080 - categorical_accuracy: 0.7785 - val_loss: 0.3988 - val_categorical_accuracy: 0.8550\n","Epoch 3/15\n","1500/1500 [==============================] - 18s 12ms/step - loss: 0.5309 - categorical_accuracy: 0.8071 - val_loss: 0.3552 - val_categorical_accuracy: 0.8761\n","Epoch 4/15\n","1500/1500 [==============================] - 19s 12ms/step - loss: 0.4868 - categorical_accuracy: 0.8290 - val_loss: 0.3160 - val_categorical_accuracy: 0.8939\n","Epoch 5/15\n","1500/1500 [==============================] - 20s 13ms/step - loss: 0.4481 - categorical_accuracy: 0.8436 - val_loss: 0.3327 - val_categorical_accuracy: 0.8771\n","Epoch 6/15\n","1492/1500 [============================>.] - ETA: 0s - loss: 0.4256 - categorical_accuracy: 0.8537"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-12d7d23ce430>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1412\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \"\"\"\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m       raise ValueError(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    316\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m       \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m     \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1157\u001b[0m     \"\"\"\n\u001b[1;32m   1158\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["epochs = 15      ## Number of epoch to run\n","batch_size = 32  ## Mini batch size\n","\n","\n","# Train the model.\n","history = model.fit(\n","  train_images, to_categorical(train_labels),\n","  epochs=epochs,\n","  batch_size=batch_size,\n","  verbose = 1,\n","  validation_data=(val_images, to_categorical(val_labels))\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"9weV4eGc9tp4"},"source":["# Explore the training progress\n","Show the training progress, by plotting the training and validation accuracy and loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jSw5Dp3f-XCw"},"outputs":[],"source":["epochrange = range(1, epochs + 1) # change to the epoch it stopped at when adding the early stopping method\n","train_acc = history.history['categorical_accuracy']\n","val_acc = history.history['val_categorical_accuracy']\n","\n","train_loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","plt.plot(epochrange, train_acc, 'bo', label='Training acc')\n","plt.plot(epochrange, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy (modell 1)')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()\n","\n","plt.plot(epochrange, train_loss, 'bo', label='Training loss')\n","plt.plot(epochrange, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss (modell 1)')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Bn2kPMlsJxpW"},"source":["Notera hur träningsnoggrannheten konstant går uppåt (och träningsförlusten nedåt). Medan valideringsförlusten bottnar ut någonstans mellan 5-10 epochs för att sedan långsamt går uppåt. Detta tyder på en svag överträning, om förlusten hade fortsatt nedåt tillsammans med träningsförlusten hade modellen fortfarande varit undertränad. Men överlag kan vi misstänka att modellens kapacitet vid 15 epochs är det bästa som denna modell kan åstadkomma."]},{"cell_type":"markdown","metadata":{"id":"_6V9rOkX-bRa"},"source":["# Evaluate the model on the test data.\n","This first model get something around 89.5 % accuracy, not bad, but we can improve on this!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16808,"status":"ok","timestamp":1668607993175,"user":{"displayName":"Hadi ammar mohammad Saghir","userId":"05748174023284280158"},"user_tz":-60},"id":"ZrwPwEQn-dVX","outputId":"c1a6b4aa-5b5c-449f-cbe7-4134fb6808af"},"outputs":[{"output_type":"stream","name":"stdout","text":["313/313 [==============================] - 1s 4ms/step - loss: 0.2220 - categorical_accuracy: 0.9221\n","Test accuracy: 0.922\n"]}],"source":["# Evaluate the model.\n","test_loss, test_acc = model.evaluate(test_images,to_categorical(test_labels))\n","print('Test accuracy: %.3f' % test_acc)"]},{"cell_type":"markdown","metadata":{"id":"oR2HHYg2AoMK"},"source":["# Övning Del 1a\n","## Frågor att besvara:\n","* Hur många parametrar har din modell?\n","* Vilken testnoggrannhet får du? (Då modellen är slumpmässing initierad kan du få ett värde som skiljer sig lite grann.)\n","* Om du tittar på träningsförloppet, har modellen tränat färdigt eller är den undertränad eller kanske övertränad, vad i träningskurvorna kan avgöra det?\n","* Hur många epochs skulle ha varit lämpligt att använda?\n","\n","Answer:\n","\n","*   The model has 201,050 parameters. \n","*   After training for 15 epochs, the test accuracy is 89% (0.894).\n","*   In order to analyze whether the model is underfitted or overfitted, there is a validation metrics help indicate the model's fitting. At 7 epochs, the validation loss is at its lowest and rises subsequently up gradually. This indicated that the model is overfitting, training to score better on the training set and not worse of new input values.\n","*   For this model (absence of regulatization except for the singluar convolution layer), 7 epochs would suffice.\n","\n","\n"," \n","\n"]},{"cell_type":"markdown","metadata":{"id":"5kyZXtIvBtQz"},"source":["# Del 1b - Fortsatta experiment:\n","Det finns nu många saker du kan utforska men vi vill att du nu undersöker lite olika modeller. Undersök om du får en förbättring av testnoggrannhet ifall du gör modellen mer kraftfull (fler noder och/eller fler lager). Ha hela tiden ett öga på ifall modellen börjar överträna (att valideringsförlusten börjar gå uppåt igen). Notera att man gärna har med MaxPooling2D-lager mellan varje eller varannat faltningslager, då det hjälper med att hålla ner antalet parametrar vilket även leder till snabbare och ofta bättre träningsresultat (ger tex en viss extra translations och skalinvarians). Detta är speciellt viktigt om man har stora inbilder, men kan nog ge bra effekt även på våra minimala bilder.\n","\n","Målet är att få modellen att bli komplex nog att väl kunna representera data. En viss överträning är OK, vi ska ju sedan åtgärda det med regularisering.\n","* Hur många faltningslager verkar rimligt att använda (håll dig under 4-5, det kan vara bättre att lägga till kärnor istället.\n","* Hur många kärnor/noder verkar vettigt att använda?\n","* Pröva även att använda ex 5x5 kärna för det första eller kanske något lager till, blir det bättre då? \n","* Om en modell övertränar, gör ett experiment där du istället för att köra alla dina epochs slutar där verifieringsfelet börjar gå uppåt igen. Vad blir skillnaden i testnoggrannhet?\n","\n","Du behöver inte ha med kod för alla experiment, om du inte vill, men vi vill åtminstone se koden, träningsförloppet, och testnoggrannheten för din bästa modell.\n","\n","För minst en av dessa parameterinställningar ska du nå mer än 90% rätt på testdata för Fashion-MNIST."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2858,"status":"ok","timestamp":1668364439382,"user":{"displayName":"Hadi ammar mohammad Saghir","userId":"05748174023284280158"},"user_tz":-60},"id":"DpSLuBimWmJk","outputId":"3be40b93-893a-4f0d-af16-fc1771a5e950"},"outputs":[{"output_type":"stream","name":"stdout","text":["Input shape (28, 28, 1)\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 28, 28, 16)        160       \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 27, 27, 16)       0         \n"," )                                                               \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 27, 27, 16)        2320      \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 26, 26, 16)       0         \n"," 2D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 10816)             0         \n","                                                                 \n"," dense (Dense)               (None, 16)                173072    \n","                                                                 \n"," dense_1 (Dense)             (None, 10)                170       \n","                                                                 \n","=================================================================\n","Total params: 175,722\n","Trainable params: 175,722\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# Kod för din bästa modell och dess träning och utvärdering\n","input_shape = test_images[0].shape\n","print(\"Input shape\", input_shape)\n"," \n","# The Keras model will be the simplest Keras model for NN networks.\n","# It is a single stack of layers connected sequentially.\n","model = Sequential([\n"," \n","# Add a convolution layer\n","Conv2D(filters=16, kernel_size=(3, 3), padding='same', activation='relu', input_shape=input_shape),\n","#Add a pooling layer\n","tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'),\n","# Add a convolution layer\n","Conv2D(filters=16, kernel_size=(3, 3), padding='same', activation='relu', input_shape=input_shape),\n","#Add a pooling layer\n","tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'),\n"," \n"," \n","# Flatten the input. This prepares the vector for fully connected layers.\n","Flatten(),\n"," \n","# Add a hidden layer\n","Dense(16, activation=\"relu\"),\n"," \n"," \n","# Add a an output layer. The output space is the number of classes\n","#    Softmax makes the output as probablity vector of the different classes\n","Dense(units=num_classes, activation='softmax')\n"," \n","])\n"," \n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"7MMttcupPmiq"},"source":["# Analys\n","### More dense layers\n","I experimented with increasing amount and size of the dense layers to (300, 50, and 10). This resulted in 3,700k parameter. The test accuracy was higher 92% but the test took approx 3m.\n","\n","I added other convolution layers as well to reduce the need for more dense layers: conv(32) > conv(64) > conv(128) > max (2,2) > dense(128) > dense(softmax). The result was 2,000k parameters with 0.923 accuracy.\n","\n","As i didn't want to increase the parameters i used other methods to achieve an over 0.9 test accuracy.\n","\n","### Best result\n","I experimented with adding 2 convolution layers [kernel size = (3,3)], 2 pooling [pool_size=(2, 2)],  1 flatter and 2 dense layers. This allowed me reduce to approx 30k less parameter compared to the original model at 201k parameter. This model scored 91%(0.911) with only 175k parameters. The started to slightly overfit around the 13th epoch, but considering that the some overfitting is OK, 15 was ideal.\n","\n","### More convolution\n","I tried adding a third convolution later with and without an extra pool. The result were nearly the same. I tried removing the pools: the validation loss shot up very quickly, around the 6th epoch. Despite that, the test accuracy was 0.9.\n","\n","### Differnt kernel size\n","I added a kernal size smaller then larger convolution layers, in accordance to the cortex's neuron( the first focus on a small local receptive field and the others focus on the larger patterns). This didn't help, but rather dropped the test accuracy to 0.906. 2 convolution layers: the first [kernel size = (2,2)] and the second [kernel size = (3,3)]. Results fluctuated between 0.906-0.910. Even though the less kernel size the less parameters, the scoring was under 0.911 (the best result).\n","\n","As suggested in the excersize, i added 2 convolution layers: the first [kernel size = (5,5)] and the second [kernel size = (3,3)]. The result was 0.905 after the first epoch but then stagnated, fluctuating between 0.90 to 0.91.\n","\n","### Differnt amount of filters\n","I experimented adding different size filters. Inspire by the LeNet-5 described in the book, i added in > conv(6) > pool > conv(16) > pool > conv(160) > flatten > dense(84) > dense(10), but i resulted 0.911 with 9,700k parameter, so a downgrade due to excess parameters. \n","\n","The LeNet-5 code verbatium from the book only resulted in an underwhelming 0.833 and 32k parameters."]},{"cell_type":"markdown","metadata":{"id":"CIR5LYQi6vDe"},"source":["# Övning Del 2a - avbruten träning\n","Utgå ifrån din bästa modell som du fått fram i del 1 och för den implementera avbruten träning (eng. early stopping) som ett sätt att förhindra överträning. [See Geron s. 141]\n","\n","Du behöver definiera en \"callback\" som sedan includeras i anropet till model.fit: *es=tf.keras.callbacks.EarlyStopping('val_loss', patience=3, restore_best_weights = True)*\n","\n","*model.fit(\n","  train_images, to_categorical(train_labels),\n","  epochs=epochs,\n","  batch_size=batch_size,\n","  **callbacks=[es]**,\n","  validation_data=(val_images, to_categorical(val_labels))\n",")*\n","\n","Viktigt är att notera restore_best_weights=True ifall man använder patience>0, för annars så har man en övertränad modell efter träningen.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":116837,"status":"ok","timestamp":1668607965109,"user":{"displayName":"Hadi ammar mohammad Saghir","userId":"05748174023284280158"},"user_tz":-60},"id":"Eilf6C_E7Mcd","outputId":"da1979bb-1726-4450-d20e-d73bad3103a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","1500/1500 [==============================] - 14s 9ms/step - loss: 0.2700 - categorical_accuracy: 0.9016 - val_loss: 0.2595 - val_categorical_accuracy: 0.9103\n","Epoch 2/15\n","1500/1500 [==============================] - 13s 9ms/step - loss: 0.2239 - categorical_accuracy: 0.9174 - val_loss: 0.2367 - val_categorical_accuracy: 0.9231\n","Epoch 3/15\n","1500/1500 [==============================] - 13s 8ms/step - loss: 0.2028 - categorical_accuracy: 0.9241 - val_loss: 0.2193 - val_categorical_accuracy: 0.9197\n","Epoch 4/15\n","1500/1500 [==============================] - 13s 9ms/step - loss: 0.1789 - categorical_accuracy: 0.9329 - val_loss: 0.2168 - val_categorical_accuracy: 0.9231\n","Epoch 5/15\n","1500/1500 [==============================] - 13s 8ms/step - loss: 0.1656 - categorical_accuracy: 0.9373 - val_loss: 0.2175 - val_categorical_accuracy: 0.9218\n","Epoch 6/15\n","1500/1500 [==============================] - 13s 9ms/step - loss: 0.1480 - categorical_accuracy: 0.9434 - val_loss: 0.2131 - val_categorical_accuracy: 0.9239\n","Epoch 7/15\n","1500/1500 [==============================] - 13s 9ms/step - loss: 0.1430 - categorical_accuracy: 0.9468 - val_loss: 0.2137 - val_categorical_accuracy: 0.9256\n","Epoch 8/15\n","1500/1500 [==============================] - 13s 8ms/step - loss: 0.1296 - categorical_accuracy: 0.9521 - val_loss: 0.2173 - val_categorical_accuracy: 0.9229\n","Epoch 9/15\n","1500/1500 [==============================] - 13s 8ms/step - loss: 0.1258 - categorical_accuracy: 0.9526 - val_loss: 0.2134 - val_categorical_accuracy: 0.9272\n"]}],"source":["# Compile the model, as a preparation for training\n","model.compile(\n","  optimizer='adam',\n","  loss='categorical_crossentropy',\n","  metrics=['categorical_accuracy']\n",")\n","\n","epochs = 15      ## Number of epoch to run\n","batch_size = 32  ## Mini batch size\n","\n","es=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights = True)\n","\n","# Train the model.\n","history = model.fit(\n","  train_images, to_categorical(train_labels),\n","  epochs=epochs,\n","  batch_size=batch_size,\n","  verbose = 1,\n","  callbacks = [es],\n","  validation_data=(val_images, to_categorical(val_labels))\n",")"]},{"cell_type":"markdown","metadata":{"id":"vaZR15OEJQKs"},"source":["## Analys\n","Vid vilken Epoch stannar nu träningen? Ändras nu modellens testnoggrannhet då den inte övertränar?\n","\n","with patient set to 5\n","\n","The model given to us averages a lower test accuracy of 0.888 compared to 0.894 when trained without early stopping. \n","\n","My model in subpart 1b has now, trained with early stopping, a more stable test accuracy at 0.912 instead of averaging sometimes 0.908. It stops at epoch 9 (3 epochs after the first increase in val_loss) "]},{"cell_type":"markdown","metadata":{"id":"xoYoC9KYTLtT"},"source":["# Övning Del 2b Andra former av regularisering\n"]},{"cell_type":"markdown","metadata":{"id":"n0PXMFPccvP8"},"source":["Utgå ifrån din bästa modell som du fått fram i del 1 men använd avbruten träning. Nu ska du skapa en modell som använder regularisering, tex. drop-out eller batchnormalisering. Analysera och jämför dessa modeller (med och utan regularisering) genom att notera deras testnoggrannhet och plotta tränings- och valideringsnoggrannheten (eng. accuracy) respektive förlust (eng. loss), och notera ifall en förbättring kan iakttas med regularisering. Hur många Epoch använder du för träningen? [See Geron s 338, s. 365]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xx3Il_R3TAjl","executionInfo":{"status":"ok","timestamp":1668607716026,"user_tz":-60,"elapsed":828,"user":{"displayName":"Hadi ammar mohammad Saghir","userId":"05748174023284280158"}},"outputId":"1248b77e-a4d9-4da4-bd51-c8572d56f506"},"outputs":[{"output_type":"stream","name":"stdout","text":["Input shape (28, 28, 1)\n","Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_15 (Conv2D)          (None, 28, 28, 64)        640       \n","                                                                 \n"," conv2d_16 (Conv2D)          (None, 28, 28, 64)        36928     \n","                                                                 \n"," max_pooling2d_6 (MaxPooling  (None, 27, 27, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_13 (Dropout)        (None, 27, 27, 64)        0         \n","                                                                 \n"," conv2d_17 (Conv2D)          (None, 27, 27, 64)        36928     \n","                                                                 \n"," max_pooling2d_7 (MaxPooling  (None, 26, 26, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_14 (Dropout)        (None, 26, 26, 64)        0         \n","                                                                 \n"," flatten_3 (Flatten)         (None, 43264)             0         \n","                                                                 \n"," dense_8 (Dense)             (None, 128)               5537920   \n","                                                                 \n"," dense_9 (Dense)             (None, 10)                1290      \n","                                                                 \n","=================================================================\n","Total params: 5,613,706\n","Trainable params: 5,613,706\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# Kod där du inför regularisering\n","input_shape = test_images[0].shape\n","print(\"Input shape\", input_shape)\n"," \n","# The Keras model will be the simplest Keras model for NN networks.\n","# It is a single stack of layers connected sequentially.\n","model = Sequential([\n","\n","# Add a convolution layer\n","Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu', input_shape=input_shape),\n","# Add a convolution layer\n","Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu', input_shape=input_shape),\n","#Add a pooling layer\n","tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'),\n","# Dropout\n","tf.keras.layers.Dropout(0.5),\n"," \n","# Add a convolution layer\n","Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu', input_shape=input_shape),\n","#Add a pooling layer\n","tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'),\n","# Dropout\n","tf.keras.layers.Dropout(0.5),\n"," \n","# Flatten the input. This prepares the vector for fully connected layers.\n","Flatten(),\n","# Add a hidden layer\n","Dense(128, activation='relu'),\n"," \n","# Add a an output layer. The output space is the number of classes\n","#    Softmax makes the output as probablity vector of the different classes\n","Dense(units=num_classes, activation='softmax')\n","])\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"Ctk9BAgVIUCD"},"source":["## Analys\n","Analysera och jämför dina modeller (med och utan regularisering) genom att notera deras testnoggrannhet och plotta tränings- och valideringsnoggrannheten (eng. accuracy) respektive förlust (eng. loss), och notera ifall en förbättring kan iakttas med regularisering. Hur många Epoch använder du för träningen? \n","\n","### 3 layers of dropout\n","With dropout, I added 2 new dense layers and increased their size greatly to compensate for the dropout: ened up with 200k parameter. The dense layers' size was 500, 300, 10 respectively.  the training accuracy/loss and validation accuracy/loss has a strong correlate: nearly same accuracy. The Without regularization, the accuracy tends to increase roughly 10% higher than validation accuracy. However, The test accuracy is averaging a measly 0.814 accurcay and trained 15 epochs.\n","\n","### 3 dense layers, 2 dropout\n","I tried to decrease the amount regularization because the accuracy was stagnating at 7 epochs and stopped training at 8 epochs. this achieve the same accuracy with only 52k parameters as also decrease the size of the dense layers to 300, 100, and 10 respectively and dropout rate (0.2). The test accuracy was 0.800.\n","\n","### 6 convolution layers with batch normalization\n","My highest yet test accuracy at 0.924 after 11 epochs and yet, kept my model to only 400k parameters.. The convoluation layer filters started with two layers with 32 filter, two layers with 64 filters, and two layers 128 filters, the three pares divided by a pooling layer. I added dense layers of sizes in the following order: 100, 50 and softmax. (This is my best model so far)\n","\n","### 3 convolution layers with dropout (found online)\n","The model had 2 convolution layers > pooling > dropped (0.5). Followed by 1 convolution layer > max pooling > dropped(0.5). Finally, 2 dense layers: 128 and softmax respectively. I was able to attain the same result of 0.924 (compared to aformentioned model) in 13 epochs but with less convolution layers, so this is my best model and will be used as a base for further testing.\n","\n","### Overboard\n","I thought i would just increase the layers to reach the point: \"alright, this has gone overboard\". 6 convolution layers, each followed by batch noramlization, and 3 dense layers. The validation accuracy dipped super low getting somewhere between 0.2 and 0.4. I added L2 regularization to my convolution and dense layers but the validation result was still so low.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ominqognCKQ0"},"source":["# Övriga Hyperparametrar\n","Det finns ett antal andra viktiga parametrar som man kan behöva justera. Men flera av dem hänger ihop, så man behöver jobba med olika modeller och data för att få en bra känsla för dem. [See Geron s. 325.., s. 351.. ]\n","* Batch size / batch storlek -- Ett större värde här kan ge GPUn mer parallellism att arbeta med, dvs snabbare, men ett för stort värde kan ge minnesproblem i GPUn och dessutom göra inlärningen långsammare (man kan dock även ha större LR i så fall.\n","* Optimizer / optimerare -- En vanlig och bra optimerade är Adam (den är dessutom ganska okänslig för vilken inlärningstakt som den startar med, vilket gör att skönsvärdet fugerar bra). Medan sgd är en mer \"ursprunglig\" optimerare.\n","* Learning rate / inlärningstakt (LR) -- Viktig parameter för hur snabbt modellen tränar, men för stort färde kan ge instabil träning."]},{"cell_type":"markdown","metadata":{"id":"H-7dDWzwGH9c"},"source":["# Övning Del 2c\n","Byt ut optimeraren från adam till sgd (med regularisering och avbruten träning). Jämför träningsförfarandet mellan de två optimerarna (skillnad i testnoggrannhet, vilken epoch stannade träningen, etc.?). \n","\n","### Answer\n","\n","I read online that sgd is better for image recognition, but this wasn't reflected it from my results. However, this could be due to me adjusting my model to best suited for Adam optimized.\n","\n","Sgd opitimized training had the following attribute: 0.906 test accuracy, stopped at 14 epochs, and val_loss began at 0.4 but dropped and stagnated at 0.23.\n","\n","Adam opitimized training had the following attribute: 0.924 test accuracy, stopped at 10 epochs, and val_loss began at 0.2s and stagnated. The result was val_acc stagnant, fluctuating between 0.921 and 0.935"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7MeBdyzAHiCX","colab":{"base_uri":"https://localhost:8080/","height":820},"executionInfo":{"status":"error","timestamp":1668368384775,"user_tz":-60,"elapsed":148375,"user":{"displayName":"Hadi ammar mohammad Saghir","userId":"05748174023284280158"}},"outputId":"63e7f70b-394b-437a-bfe3-f633442eb2e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","1500/1500 [==============================] - 14s 8ms/step - loss: 0.6451 - categorical_accuracy: 0.7621 - val_loss: 0.4516 - val_categorical_accuracy: 0.8541\n","Epoch 2/15\n","1500/1500 [==============================] - 12s 8ms/step - loss: 0.4106 - categorical_accuracy: 0.8510 - val_loss: 0.3671 - val_categorical_accuracy: 0.8802\n","Epoch 3/15\n","1500/1500 [==============================] - 14s 9ms/step - loss: 0.3576 - categorical_accuracy: 0.8709 - val_loss: 0.3600 - val_categorical_accuracy: 0.8860\n","Epoch 4/15\n","1500/1500 [==============================] - 13s 8ms/step - loss: 0.3296 - categorical_accuracy: 0.8803 - val_loss: 0.3243 - val_categorical_accuracy: 0.8871\n","Epoch 5/15\n","1500/1500 [==============================] - 12s 8ms/step - loss: 0.3063 - categorical_accuracy: 0.8891 - val_loss: 0.2958 - val_categorical_accuracy: 0.8978\n","Epoch 6/15\n","1500/1500 [==============================] - 12s 8ms/step - loss: 0.2883 - categorical_accuracy: 0.8940 - val_loss: 0.2783 - val_categorical_accuracy: 0.9011\n","Epoch 7/15\n","1500/1500 [==============================] - 12s 8ms/step - loss: 0.2753 - categorical_accuracy: 0.8995 - val_loss: 0.2713 - val_categorical_accuracy: 0.9020\n","Epoch 8/15\n","1500/1500 [==============================] - 13s 8ms/step - loss: 0.2596 - categorical_accuracy: 0.9050 - val_loss: 0.2625 - val_categorical_accuracy: 0.9071\n","Epoch 9/15\n","1500/1500 [==============================] - 12s 8ms/step - loss: 0.2507 - categorical_accuracy: 0.9073 - val_loss: 0.2514 - val_categorical_accuracy: 0.9119\n","Epoch 10/15\n","1500/1500 [==============================] - 12s 8ms/step - loss: 0.2381 - categorical_accuracy: 0.9129 - val_loss: 0.2594 - val_categorical_accuracy: 0.9123\n","Epoch 11/15\n","1500/1500 [==============================] - 12s 8ms/step - loss: 0.2294 - categorical_accuracy: 0.9153 - val_loss: 0.2479 - val_categorical_accuracy: 0.9155\n","Epoch 12/15\n","1237/1500 [=======================>......] - ETA: 1s - loss: 0.2213 - categorical_accuracy: 0.9196"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-fde0ba7e4b7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1412\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \"\"\"\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m       raise ValueError(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    316\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m       \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m     \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1157\u001b[0m     \"\"\"\n\u001b[1;32m   1158\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Din kod för en körning med sgd som optimerare\n","# Compile the model, as a preparation for training\n","model.compile(\n","  optimizer='sgd',\n","  loss='categorical_crossentropy',\n","  metrics=['categorical_accuracy']\n",")\n","\n","epochs = 15      ## Number of epoch to run\n","batch_size = 32  ## Mini batch size\n","\n","es=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights = True)\n","\n","# Train the model.\n","history = model.fit(\n","  train_images, to_categorical(train_labels),\n","  epochs=epochs,\n","  batch_size=batch_size,\n","  verbose = 1,\n","  callbacks = [es],\n","  validation_data=(val_images, to_categorical(val_labels))\n",")"]},{"cell_type":"markdown","metadata":{"id":"d7QvZC8ONS95"},"source":["# Del 3 Auto tune\n","Here we test on of the hyperparameter optimizers called Keras Tuner. Documentation can be found here: https://keras-team.github.io/keras-tuner/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5094,"status":"ok","timestamp":1668387672330,"user":{"displayName":"Hadi ammar mohammad Saghir","userId":"05748174023284280158"},"user_tz":-60},"id":"mCL16yCINhVS","outputId":"5c528aab-a214-46b2-fb67-7d1402d0e59d"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 135 kB 5.0 MB/s \n","\u001b[K     |████████████████████████████████| 1.6 MB 40.7 MB/s \n","\u001b[?25hKeras Tuner version: 1.1.3\n"]}],"source":["# Get Keras Tuner (if in colab)\n","import os\n","\n","\n","!pip install keras-tuner -q;\n","\n","import keras_tuner\n","print('Keras Tuner version:', keras_tuner.__version__)\n","\n","# Get some tuner search functions\n","from keras_tuner.tuners import BayesianOptimization"]},{"cell_type":"markdown","metadata":{"id":"PVNB09pdRwo1"},"source":["## Exempelmodell (du ska sedan göra en egen!)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DkN7ntftOVQi"},"outputs":[],"source":["# This is a straight forward CNN model to decently solve Fashion MNIST\n","# Note that we have no regularisation for this example!\n","# It can at least achieve 92% accuracy for me, with the parameters found:\n","# conv_1_filter =  96, conv_1_kernel =   5, \n","# conv_2_filter =  48, conv_2_kernel =   5\n","# dense_1_units =  128\n","# learning_rate = 0.001\n","# batch_size = 320\n","\n","def build_model_2Conv1Dense(hp):  \n","  model = tf.keras.Sequential([\n","    # First Convolutional Layer\n","    tf.keras.layers.Conv2D(\n","        filters=hp.Int('conv_1_filter', min_value=32, max_value=256, step=32),\n","        kernel_size=hp.Choice('conv_1_kernel', values = [3,5]),\n","        activation='relu',\n","        padding='same',\n","        input_shape=(28,28,1)\n","    ),\n","    tf.keras.layers.BatchNormalization(),\n","\n","    # Second Convolutional Layer\n","    tf.keras.layers.Conv2D(\n","        filters=hp.Int('conv_2_filter', min_value=32, max_value=256, step=32),\n","        kernel_size=hp.Choice('conv_2_kernel', values = [3,5]),\n","        activation='relu',\n","        padding='same'\n","    ),\n","    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)),\n","\n","    # A First Dense Layer\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(\n","        units=hp.Int('dense_1_units', min_value=32, max_value=256, step=32),\n","        activation='relu'\n","    ),\n","\n","    # A Final Dense Layer\n","    tf.keras.layers.Dense(10, activation='softmax')\n","  ])\n","  \n","  model.compile(optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3])),\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","  \n","  return model"]},{"cell_type":"markdown","metadata":{"id":"evRPRGRkOmpm"},"source":["# Övning 3: Kod för instrumentera din egen model att optimeras med Keras Tuner\n","Utgå ifrån din bästa modell som du fått fram i del 1 och instrumentera den för att optimeras med Keras Tuner."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Haf7S-sOzKs"},"outputs":[],"source":["input_shape = test_images[0].shape\n","\n","# Kompletera denna kod med din modell\n","def build_model_MyModelOne(hp):  \n","  model = tf.keras.Sequential([\n","      # Add a convolution layer\n","      tf.keras.layers.Conv2D(\n","        filters=hp.Int('conv_1_filter', min_value=32, max_value=256, step=32),\n","        kernel_size=hp.Choice('conv_1_kernel', values = [3,5]),\n","        activation='relu',\n","        padding='same',\n","        input_shape=(28,28,1)\n","    ),\n","      # Add a convolution layer\n","      tf.keras.layers.Conv2D(\n","        filters=hp.Int('conv_2_filter', min_value=32, max_value=256, step=32),\n","        kernel_size=hp.Choice('conv_2_kernel', values = [3,5]),\n","        activation='relu',\n","        padding='same',\n","        input_shape=(28,28,1)\n","    ),\n","      #Add a pooling layer\n","      tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'),\n","      # Dropout\n","      tf.keras.layers.Dropout(0.5),\n","      \n","      \n","      # Add a convolution layer\n","      tf.keras.layers.Conv2D(\n","        filters=hp.Int('conv_3_filter', min_value=32, max_value=256, step=32),\n","        kernel_size=hp.Choice('conv_3_kernel', values = [3,5]),\n","        activation='relu',\n","        padding='same',\n","        input_shape=(28,28,1)\n","    ),\n","      #Add a pooling layer\n","      tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'),\n","      # Dropout\n","      tf.keras.layers.Dropout(0.5),\n","      \n","      # Flatten the input. This prepares the vector for fully connected layers.\n","      Flatten(),\n","      # Add a hidden layer\n","      tf.keras.layers.Flatten(),\n","        tf.keras.layers.Dense(\n","            units=hp.Int('dense_1_units', min_value=32, max_value=256, step=32),\n","            activation='relu'\n","      ),\n","\n","      # A Final Dense Layer\n","      tf.keras.layers.Dense(10, activation='softmax')\n","    ])\n","  \n","  ##### You can also try some other learning rates in the next line, or use another optimizer with other parameters\n","  model.compile(optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3])),\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","  \n","  return model"]},{"cell_type":"markdown","metadata":{"id":"RGMc2PtqWeoY"},"source":["# Do the actual search, based on a BayesianOptimization\n","Widely-used tuning algorithms: RandomSearch, BayesianOptimization and Hyperband. Here we will use BayesianOptimization. \n","\n","But note that there is a parameter *num_initial_points* to BayesianOptimization which state the number of randomly generated samples as initial training data for Bayesian optimization. If left unspecified, a value of 3 times the dimensionality of the hyperparameter space is used. That is, we might only use random search if we have too few trails."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"10H7acwkPOpB"},"outputs":[],"source":["# To get more stable results we are repeating the runs two times for each parameter setup\n","MAX_TRIALS = 20         # represents the number of hyperparameter combinations that will be tested by the tuner\n","EXECUTION_PER_TRIAL = 2 # the number of models that should be built and fit for each trial for robustness purposes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mw62ioE_PQKZ"},"outputs":[],"source":["# To have batch_size as a hyperparameter we need to define our own tuner\n","# In this case, we are basing it on the BayesianOptimization tuner found in\n","# https://keras-team.github.io/keras-tuner/documentation/tuners/#bayesianoptimization-class\n","class MyTuner(keras_tuner.tuners.BayesianOptimization):\n","  def run_trial(self, trial, *args, **kwargs):\n","    # You can add additional HyperParameters for preprocessing and custom training loops\n","    # via overriding `run_trial`\n","    kwargs['batch_size'] = trial.hyperparameters.Int('batch_size', 32, 512, step=32, default=256)\n","    return super(MyTuner, self).run_trial(trial, *args, **kwargs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tMC8hgl3PZpI","executionInfo":{"status":"ok","timestamp":1668389225054,"user_tz":-60,"elapsed":211,"user":{"displayName":"Hadi ammar mohammad Saghir","userId":"05748174023284280158"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9959f544-6c42-452f-9a4f-1a28b88ea4f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["JAg körde\n"]}],"source":["# Bayesian search which also searches for batch_size\n","tuner = MyTuner(\n","    build_model_MyModelTwo, ##### Laboration: Change this to 'build_model_MyModel' #####\n","    max_trials=MAX_TRIALS,\n","    objective='val_accuracy',\n","    executions_per_trial=EXECUTION_PER_TRIAL,\n","    directory='output',\n","    overwrite=True,\n","    num_initial_points=10, # Start with 10 random points and then do more structured search\n","    project_name='FashionMNIST'\n",")"]},{"cell_type":"markdown","metadata":{"id":"7AqPBQEPfCw1"},"source":["# Help it takes so much time!\n","Yes, doing a parameter search takes time! Using a K80 this search takes 1h24 (while a RTX 3090 takes 10 minutes), and if your model is more complicated it can take even more time. To handle this you could try setting EXECUTION_PER_TRIAL=1, even if the search becomes more instable. You can also try to run in phases, do a first run with EXECUTION_PER_TRIAL=1 and even noepochauto = 4 or 5, to get a feeling for good parameter limits and starting points (setting things like \"default=128\" in the hp.Int() call) for your variables, and maybe even lock some variables (like LR?). And then do a more focused search where you maybe try EXECUTION_PER_TRIAL=2, noepochauto = 6 or 7 for a smaller number of MAX_TRIALS.\n","\n","If you do such multiphase process, please document it well!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2360560,"status":"ok","timestamp":1668391587800,"user":{"displayName":"Hadi ammar mohammad Saghir","userId":"05748174023284280158"},"user_tz":-60},"id":"gEDRhLr8PdfO","outputId":"7bf7bb3f-ec16-48c6-d26d-2a0bae80f245"},"outputs":[{"output_type":"stream","name":"stdout","text":["Trial 20 Complete [00h 02m 44s]\n","val_accuracy: 0.9268749952316284\n","\n","Best val_accuracy So Far: 0.9301249980926514\n","Total elapsed time: 00h 39m 20s\n"]}],"source":["# Do the hyperparameter search\n","# I set the no of epochs to 6 to speed up the search in this excersise, better would have been 10\n","noepochauto = 6   ## Max number of epochs per trail (but we have early stopping so this max is probably not reached)\n","es = tf.keras.callbacks.EarlyStopping('val_loss', patience=2, restore_best_weights = True)\n","# Note we do not need to_catagorical as we use loss='sparse_categorical_crossentropy'\n","tuner.search(train_images, train_labels, \n","             epochs=noepochauto, \n","             validation_data=(val_images, val_labels), \n","             callbacks=[es])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":706,"status":"ok","timestamp":1668391673874,"user":{"displayName":"Hadi ammar mohammad Saghir","userId":"05748174023284280158"},"user_tz":-60},"id":"n-VT_1v8PsjH","outputId":"56254ef1-8039-4b0a-b6e4-0c8e738dc45c"},"outputs":[{"output_type":"stream","name":"stdout","text":["JAg körde\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 28, 28, 16)        160       \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 26, 26, 112)       16240     \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 25, 25, 112)      0         \n"," )                                                               \n","                                                                 \n"," dropout (Dropout)           (None, 25, 25, 112)       0         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 23, 23, 112)       113008    \n","                                                                 \n"," average_pooling2d (AverageP  (None, 22, 22, 112)      0         \n"," ooling2D)                                                       \n","                                                                 \n"," flatten (Flatten)           (None, 54208)             0         \n","                                                                 \n"," dense (Dense)               (None, 496)               26887664  \n","                                                                 \n"," dense_1 (Dense)             (None, 10)                4970      \n","                                                                 \n","=================================================================\n","Total params: 27,022,042\n","Trainable params: 27,022,042\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# Get the best model so far\n","model = tuner.get_best_models(num_models=1)[0]\n","# Dump the best hyperparameters found\n","vals = tuner.get_best_hyperparameters(num_trials=5)[4].values\n","#for keys,values in vals.items():\n","   # print('%20s = %3.4g' %(keys,values))\n","    \n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1976,"status":"ok","timestamp":1668392691041,"user":{"displayName":"Hadi ammar mohammad Saghir","userId":"05748174023284280158"},"user_tz":-60},"id":"Qoom8_ADRJ87","outputId":"9bf141f5-585a-49a2-ffb2-8f2a26f2b192"},"outputs":[{"output_type":"stream","name":"stdout","text":["313/313 [==============================] - 2s 5ms/step - loss: 0.2165 - accuracy: 0.9253\n","Test accuracy: 0.925\n"]}],"source":["# Evaluate the model.\n","test_loss, test_acc = model.evaluate(test_images,test_labels)\n","print('Test accuracy: %.3f' % test_acc)"]},{"cell_type":"markdown","metadata":{"id":"PxuExn6aP3RB"},"source":["## Code to explore the n-best models"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15345,"status":"ok","timestamp":1668379231722,"user":{"displayName":"Hadi ammar mohammad Saghir","userId":"05748174023284280158"},"user_tz":-60},"id":"V98UosuJQB83","outputId":"b5ccada7-dba0-460e-b1eb-b1d4d145783b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test accuracy   conv_1_filter   conv_1_kernel   conv_2_filter   conv_2_kernel   conv_3_filter   conv_3_kernel   dense_1_units   learning_rate      batch_size \n","\n","          0.928              64               3             256               5             128               3             192           0.001             288 \n","          0.923             256               3             224               5             128               5              96           0.001             192 \n","          0.921             192               3             160               5              32               3             224           0.001              32 \n","          0.923              64               3             224               5             192               5             160           0.001              64 \n","          0.921             160               5             224               5             128               5             160           0.001             256 \n"]}],"source":["# Let us dump the n-best\n","\n","if True:\n","  explorenbest = 5\n","\n","  # Suppress warnings about optimizer state not being restored by tf.keras.\n","  tf.get_logger().setLevel('ERROR')\n","\n","  # Print a heading\n","  vals = tuner.get_best_hyperparameters(num_trials=explorenbest)[0].values\n","  print('Test accuracy ', end = '')\n","  for keys,values in vals.items():\n","      print('%15s ' %(keys), end = '')\n","  print('\\n')\n","\n","  # Now print each trail on a seperate row from best to worst\n","  for ix in range(0,explorenbest): \n","    # evaluate this trail\n","    model = tuner.get_best_models(num_models=explorenbest)[ix]\n","    test_loss, test_acc = model.evaluate(test_images,test_labels, verbose=0)\n","    print('%15.3f ' % test_acc , end = '')\n","    # get this trail's hyperparameters\n","    vals = tuner.get_best_hyperparameters(num_trials=explorenbest)[ix].values\n","    for keys,values in vals.items():\n","      print('%15.4g ' %(values), end = '')\n","    # end this line and start the trail\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"FRG79o5edPAi"},"source":["Notice that the model with the best validation accuracy not necessarily gives the best test accuracy! But we have to live with this as this is the best we can do with the data set aside for training and validation. "]},{"cell_type":"markdown","metadata":{"id":"Ix2PgTMaNPaT"},"source":["# With our found hyperparameters, continue training\n","(We do this especially as we only done 6 epochs during search.)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39757,"status":"ok","timestamp":1668392885032,"user":{"displayName":"Hadi ammar mohammad Saghir","userId":"05748174023284280158"},"user_tz":-60},"id":"74oCuObpNkPe","outputId":"577a3785-cea7-4a4b-c138-a6e76091d3d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["JAg körde\n","Epoch 1/30\n","250/250 [==============================] - 14s 53ms/step - loss: 0.0997 - accuracy: 0.9625 - val_loss: 0.1979 - val_accuracy: 0.9326\n","Epoch 2/30\n","250/250 [==============================] - 13s 52ms/step - loss: 0.0757 - accuracy: 0.9730 - val_loss: 0.2309 - val_accuracy: 0.9302\n","Epoch 3/30\n","249/250 [============================>.] - ETA: 0s - loss: 0.0570 - accuracy: 0.9789Restoring model weights from the end of the best epoch: 1.\n","250/250 [==============================] - 12s 49ms/step - loss: 0.0569 - accuracy: 0.9789 - val_loss: 0.2440 - val_accuracy: 0.9286\n","Epoch 3: early stopping\n"]}],"source":["# Fine-tune the model using the best parameters found as we might not be fully trained\n","model = tuner.get_best_models(num_models=1)[0]\n","\n","epochs = 30      ## (max) number of epoch to run\n","opt_batch_size = tuner.get_best_hyperparameters()[0]['batch_size']\n","\n","# Set callback functions to early stop training\n","callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=2, restore_best_weights = True)]\n","\n","# Continue to train the model. (note that we are continuing from the training done during tuning)\n","history = model.fit(\n","  train_images, train_labels,\n","  epochs=epochs,\n","  batch_size=opt_batch_size,\n","  verbose = 1,\n","  validation_data=(val_images, val_labels),\n","  # initial_epoch=noepochauto, ## how to get this number???? from... \"(root).optimizer.iter\" maybe\n","  callbacks=callbacks\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2865,"status":"ok","timestamp":1668392891592,"user":{"displayName":"Hadi ammar mohammad Saghir","userId":"05748174023284280158"},"user_tz":-60},"id":"bAsos-f4Nye8","outputId":"b92567c0-b47d-4431-adbc-a2e05f6244dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["313/313 [==============================] - 1s 4ms/step - loss: 0.2222 - accuracy: 0.9278\n","Test accuracy: 0.928\n"]}],"source":["# Evaluate the model.\n","test_loss, test_acc = model.evaluate(test_images,test_labels)\n","print('Test accuracy: %.3f' % test_acc)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":292,"status":"ok","timestamp":1668350443255,"user":{"displayName":"Hadi ammar mohammad Saghir","userId":"05748174023284280158"},"user_tz":-60},"id":"WketomR6iVWk","outputId":"39ef60b9-1f13-44b8-aec9-f21302ab176f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 28, 28, 64)        640       \n","                                                                 \n"," batch_normalization (BatchN  (None, 28, 28, 64)       256       \n"," ormalization)                                                   \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 28, 28, 160)       256160    \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 14, 14, 160)      0         \n"," )                                                               \n","                                                                 \n"," flatten (Flatten)           (None, 31360)             0         \n","                                                                 \n"," dense (Dense)               (None, 256)               8028416   \n","                                                                 \n"," dense_1 (Dense)             (None, 10)                2570      \n","                                                                 \n","=================================================================\n","Total params: 8,288,042\n","Trainable params: 8,287,914\n","Non-trainable params: 128\n","_________________________________________________________________\n"]}],"source":["# See what we got and how many parameters are used\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"qQKU8ZEXQZDV"},"source":["#Analys\n","\n","### frågorna\n","Gör en komplett analys av de optimerade modellerna du fått fram mha Keras Tuner. Några frågor man kan ställa sig:\n","* Hur jämför sig dessa värden, mha en Bayesiansk sökning, från de värden då fått fram i del 1 och del 2? \n","* Vilka prestanda fick du för de olika fallen? \n","* Finns det parametrar som ligger vid ändvärdena av den använda sökrymden? (i så fall kanske du borde köra om med utökade gränser?)\n","* Vilka hyperparametrar verkade vara mest betydelsefulla? \n","* Kan man se någon trend bland parametrarna (tex antalet faltningskärnor i tidiga respektive sena lager, etc)? \n","* Varför utför man samma experiment flera gångar \"EXECUTION_PER_TRIAL = 2\", dvs varför blir det inte samma resultat varje gång?\n","* Annat som du iaktagit?\n","\n","## Answers\n","\n","The model generated from bayesian searching had many more parameters: 17,300k compared to 5,600k. This is manly due a dense layer of kernel size 192 compared to 128. \n","\n","The kernel sizes of all there convolution laters were are (5,5) compared to my (3,3) in part Additionly, the filter size of the second convolution layer (out of 3) is 256 and the third convolution later is 128, compared to 64. \n","\n","The preformance improved a fair amount, landing at 0.928 compared to 0.924.\n","\n","If i would run again the test, i would increase the choice of kernel size for the convolution layers to 3, 5 and 7. This is mainly due to better preformance was always recorded when the second convolution layer had a kernel size of (5,5). As mentioned before, i based (at least attempt to) my model on the the neuron structure of the coretex. so this wasn't a suprised as i already had the second convolution layer kernel size set to (5,5). However, what happens when try larger kernel size would be interesting.\n","\n","The mode usefull parameters seemed to be the comboination of the thirs convolution layers. \n","\n","The trend that i can spot is the convolution layer 1 and 2's relationship. Otherwise the other hyperparameters seem to be somewhat random combinations.\n","\n","The result may vary as the training process is random and an execution per trail can be a way to offset luck in case we got a good model but an unlucky fitting(training session).\n","\n","\n","The one major thing i notice is how high density tends to dominate bayesian seaches. It doesn't take into consideration the training time and just focuses on which model has thhe highest accuracy. When i manually did the search i tried to adjust to only 170k parameters in my first model in 1b so i would keep the training time short. This is somewhat mitigated by the ability to adjust the bounds of hyperparameters and \"explore the n best models\""]},{"cell_type":"markdown","metadata":{"id":"V67KzZlmckBw"},"source":["# Uppgifter för väl godkänt"]},{"cell_type":"markdown","metadata":{"id":"nDD_rMGXcsyX"},"source":["##Utökning av Del2:\n","Utforska de testfall som missklassificerades för en av dina toppmodeller, vilka typer av plagg verkar vara de som oftast blir fel på, om du tittar på dessa fel förstår du varför nätet har problem med dem?"]},{"cell_type":"code","source":["##plotting the confusion matrix (copied from kaggle)\n","import itertools\n","\n","def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","    \n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=90)\n","    plt.yticks(tick_marks, classes)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, cm[i, j],\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","\n","\n","from sklearn.metrics import confusion_matrix\n","\n","# Predict the values from the validation dataset and convert to one hot vectors \n","Y_pred = np.argmax(model.predict(val_images), axis = 1) \n","# compute the confusion matrix\n","confusion_mtx = confusion_matrix(val_labels, Y_pred) \n","# plot the confusion matrix\n","plot_confusion_matrix(confusion_mtx, \n","            classes = ['T-shirt/Top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle Boot'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":329},"id":"d-IuLl6OtfUJ","executionInfo":{"status":"ok","timestamp":1668372449098,"user_tz":-60,"elapsed":2216,"user":{"displayName":"Hadi ammar mohammad Saghir","userId":"05748174023284280158"}},"outputId":"7f8425e0-d38e-4c5a-fdc7-1b93eb7f7da0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["375/375 [==============================] - 1s 3ms/step\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","print(classification_report(val_labels, Y_pred, target_names = class_names))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3J65RXeWxnP6","executionInfo":{"status":"ok","timestamp":1668372986845,"user_tz":-60,"elapsed":9,"user":{"displayName":"Hadi ammar mohammad Saghir","userId":"05748174023284280158"}},"outputId":"17f9e737-a77c-4287-8d70-00ba458c8e50"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n"," T-shirt/top       0.90      0.87      0.89      1243\n","     Trouser       0.99      0.99      0.99      1192\n","    Pullover       0.86      0.91      0.89      1196\n","       Dress       0.94      0.92      0.93      1234\n","        Coat       0.88      0.90      0.89      1157\n","      Sandal       0.99      0.99      0.99      1192\n","       Shirt       0.80      0.78      0.79      1224\n","     Sneaker       0.96      0.97      0.97      1227\n","         Bag       0.98      0.99      0.99      1133\n","  Ankle boot       0.97      0.97      0.97      1202\n","\n","    accuracy                           0.93     12000\n","   macro avg       0.93      0.93      0.93     12000\n","weighted avg       0.93      0.93      0.93     12000\n","\n"]}]},{"cell_type":"markdown","source":["### Answer\n","Accoridng to the confusion matrix, shirts are often confused with t-shirts/tops. Confusion according to the classification rapport percision seems to be lowest amongst similar looking classes: t-shirts, pullover, coat and shirts. however, this isnt a problem to amongs sneakers and ankle boots and thats probabilt because on the larger field, the patterns are easier to distiguish. \n","\n","Thus, we can conclude that the small receptive fields are needed to better distinguish the finer details that goes into distinguishing between these similar looking classes."],"metadata":{"id":"8b_PqemEw1I7"}},{"cell_type":"markdown","metadata":{"id":"81rFfKrm9HaC"},"source":["###Utökning av Del3:\n","Implementera en undersökning där du även varierar antal lager i din modell, du kan ändra både faltningslager och kompakta lager (eng. dense layers) för att se ifall en ännu bättre modell kan hittas."]},{"cell_type":"markdown","metadata":{"id":"7lQPNnmYdNBR"},"source":["Tips: Det är nog bäst att använda en inkrementell/funktionell definition av modellen (där lager läggs till med \"model.add(...)\" för då blir det lättare att göra en for-slinga runt koden som lägger till ett visst antal lager. Jmf https://keras.io/guides/sequential_model/ \n","\n","\n","```\n","    filter1=hp.Int('conv_pre_filter', min_value=32, max_value=160, step=32, default=64)\n","    kernel1=hp.Choice('conv_pre_kernel', values = [3,5], default=3)\n","    model = Sequential()\n","    # Add a first convolution layer (with input size)\n","    model.add(Conv2D(filters=filter1, kernel_size=kernel1, padding='same', activation='relu', input_shape=train_images[0].shape))\n","    # More fixed layers?\n","    model.add(....more layer definitions....) \n","    # Add a varying number of layers\n","    no_layers = hp.Int('num_layers', 0, 3)\n","    for i in range(no_layers):\n","          model.add(Conv2D(filters=hp.Int(f'conv_{i+1}_units', ...parameters...), \n","                          kernel_size=hp.Choice(f'conv_{i+1}_kernel', ...parameters....),\n","                          activation='relu'\n","                          )\n","                    )\n","\n","```\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JR43zXtw9VSx"},"outputs":[],"source":["# Din kod för en keras tuner modell som har antalet lager som en hyperparameter \n","# och där du söker efter optimerade hyperparametrar för denna modell.\n","\n","input_shape = (28,28,1)\n","\n","def build_model_MyModelTwo(hp):  \n","\n","  model = Sequential()\n","  # Add a first convolution layer (with input size)\n","  model.add(Conv2D(filters= 16, kernel_size = (3,3) , padding='same', activation='relu', input_shape=(28,28,1)))\n","\n","  # Add a varying number of layers\n","  for i in range(1, hp.Int(\"num_layers 1\", 1, 2)):\n","    model.add(Conv2D(filters=hp.Int(f'conv_{i+1}_units', min_value=16, max_value=128, step=32, default=64), \n","                    kernel_size=hp.Choice(f'conv_{i+1}_kernel', values = [3,5], default=3),\n","                    activation='relu',\n","                    input_shape = (28,28,1)\n","                    ))\n","                \n","  #Add a pooling layer\n","  if hp.Choice('pooling_1', ['avg', 'max']) == 'max':\n","    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'))\n","  else:\n","    model.add(tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'))\n","\n","  # Tune dropout layer with values from 0 - 0.3 with stepsize of 0.1.\n","  model.add(tf.keras.layers.Dropout(hp.Float(f\"dropout_perm1\", 0, 0.5, step=0.1)))\n","\n","  # Add a varying number of layers\n","  for i in range(1, hp.Int(\"num_layers 2\", 1, 2)):\n","    model.add(Conv2D(filters=hp.Int(f'conv_{i+1}_units', min_value=16, max_value=128, step=32, default=64), \n","                    kernel_size=hp.Choice(f'conv_{i+1}_kernel', values = [3,5], default=3),\n","                    activation='relu',\n","                    input_shape = (28,28,1)\n","                    ))\n","  #Add a pooling layer\n","  if hp.Choice('pooling_2', ['avg', 'max']) == 'max':\n","    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'))\n","  else:\n","    model.add(tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'))\n","\n","  #Add a flatten\n","  model.add(tf.keras.layers.Flatten())\n","\n","  # Tune the number of hidden layers and units in each.\n","  # Number of hidden layers: 1 - 2\n","  # Number of Units: 32 - 512 with stepsize of 32\n","  for i in range(1, hp.Int(\"num_layers 3\", 1, 2)):\n","    model.add(\n","        tf.keras.layers.Dense(\n","            units=hp.Int(f\"units_{i+1}\", min_value=16, max_value=512, step=32),\n","            activation=\"relu\"))\n","\n","  # A Final Dense Layer\n","  model.add(tf.keras.layers.Dense(10, activation='softmax'))\n","  print(\"JAg körde\")\n","\n","  ##### You can also try some other learning rates in the next line, or use another optimizer with other parameters\n","  model.compile(optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3])),\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","  return model\n"]},{"cell_type":"markdown","source":["##Analys\n","\n","My first trail will be be to change the kernel sizes. I would like to recall my analysis from 1b:\n","\n","> \"I added a kernal size smaller then larger convolution layers, in accordance to the cortex's neuron( the first focus on a small local receptive field and the others focus on the larger patterns).\"\n","\n","I will start with the model in 3. The best model i was able to achieve was \n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","    \n","\n","*    conv2d (Conv2D)             (None, 28, 28, 16)        160   \n","*   conv2d_1 (Conv2D)           (None, 26, 26, 112)       16240 \n","\n","* max_pooling2d (MaxPooling2D  (None, 25, 25, 112)      0                                                                      \n","                                                                 \n","* dropout (Dropout)           (None, 25, 25, 112)       0         \n","                                                                 \n","* conv2d_2 (Conv2D)           (None, 23, 23, 112)       113008    \n","                                                                 \n","* average_pooling2d (AverageP  (None, 22, 22, 112)      0         \n"," ooling2D)                                                       \n","                                                                 \n","* flatten (Flatten)           (None, 54208)             0         \n","                                                                 \n","* dense (Dense)               (None, 496)               26887664  \n","                                                                 \n","* dense_1 (Dense)             (None, 10)                4970      \n","                                                                 \n","\n","Total params: 27,022,042\n","\n","Trainable params: 27,022,042\n","\n","Non-trainable params: 0\n","_________________________________________________________________\n","\n","Test accuracy: 0.931\n","\n","\n","Session timed out before running the code for the n best and saving other information about the hyperparameters such as kernel size and dropout rate.\n"],"metadata":{"id":"VrXngQFCzYxu"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1Go8dX3Vh4spx7VnrUDD3CRgjHD-gUPXX","timestamp":1667916218339},{"file_id":"1U0SrfXMLij2DeDTds1R3Ukawxxg3kGLD","timestamp":1586115414489},{"file_id":"1K4T-fqx6afRZdTRuy-Yjb1UC_SQ8AvLD","timestamp":1586113083917}],"collapsed_sections":["948BTPYO5BI_","vaZR15OEJQKs","Ctk9BAgVIUCD","8b_PqemEw1I7","VrXngQFCzYxu"]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}